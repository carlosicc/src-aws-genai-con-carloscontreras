{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting and Chain-of-thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Algunas ideas se tomaron de este [notebook](https://github.com/aws-samples/amazon-nova-samples/blob/main/multimodal-understanding/workshop/2-prompting-best-practices.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDK & Client. See region!\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_nova(\n",
    "    model: str,\n",
    "    messages: List[Dict],\n",
    "    system_message: str = \"\",\n",
    "    max_tokens: int = 512,\n",
    "    temp: float = 0.7,\n",
    "    top_p: float = 0.99,\n",
    "    top_k: int = 20,\n",
    "    stop_sequences: List[str] = [],\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Call Nova model through AWS Bedrock for text generation or streaming.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare request body\n",
    "        request_body = {\n",
    "            \"messages\": messages,\n",
    "            \"system\": [{\"text\": system_message}],\n",
    "            \"inferenceConfig\": {\n",
    "                \"max_new_tokens\": max_tokens,\n",
    "                \"top_p\": top_p,\n",
    "                \"top_k\": top_k,\n",
    "                \"temperature\": temp,\n",
    "                \"stopSequences\": stop_sequences,\n",
    "            }\n",
    "        }\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model, \n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        \n",
    "        return model_response, model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error calling Nova model: {str(e)}\")\n",
    "    \n",
    "def print_output(content_text):\n",
    "    display(Markdown(content_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Elige tu modelo, acorde a tu regiÃ³n: https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html \n",
    "# - Intenta con otros modelos de Amazon: us.amazon.nova-micro-v1:0, us.amazon.nova-lite-v1:0, us.amazon.nova-pro-v1:0\n",
    "model_id = \"us.amazon.nova-micro-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shot = \"\"\"Your task is to Classify the following texts into the appropriate sentiment classes. The categories to classify are:\n",
    "\n",
    "Sentiment Classes:\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "\n",
    "Query:\n",
    "Input: The movie makes users think about their lives with the teenagers while still making audience unclear on the storyline.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": no_shot}]}]\n",
    "\n",
    "model_response, content_text = call_nova(model_id, messages)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AÃ±adamos \"a few shot\" al prompt!\n",
    "> Intenta cambiar el prompt; e.g. \"The movie is only good if you need a nap in the theatre.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_shot = \"\"\"Your task is to Classify the following texts into the appropriate sentiment classes. The categories to classify are:\n",
    "\n",
    "Sentiment Classes:\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "\n",
    "Please refer to some examples mentioned below.\n",
    "\n",
    "## Examples\n",
    "### Example 1\n",
    "Input: The movie was crazy good! I loved it\n",
    "Output: Positive\n",
    "Explaination: The text said \"good\" and \"loved\" so its positive\n",
    "\n",
    "### Example 2\n",
    "Input: The movie was scary and I got scared!\n",
    "Output: Neutral\n",
    "Explaination: The text said \"scary\" and \"scared\" which can be both positive and negative depending on people who like scary movies or one who hate\n",
    "\n",
    "### Example 3\n",
    "Input: The movie was pathetic not worth the time or money!\n",
    "Output: Negative\n",
    "Explaination: The text said \"pathetic\" and \"not worth\" which is negative sentiment\n",
    "\n",
    "### Example 4\n",
    "Input: The movie had some plots which were interesting and great while there were some gaps which needed more drama!\n",
    "Output: Neutral\n",
    "Explaination: The text said \"interesting and great\" and \"some gaps\" making it a mixed opinion hence neutral\n",
    "\n",
    "Query:\n",
    "Input: Just watch a different movie. Try maybe one from Sandra Bullock ðŸ¤¬\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": four_shot}]}]\n",
    "\n",
    "model_response, content_text = call_nova(model_id, messages)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â User Input\n",
    "no_cot = \"\"\"In a room of 100 people, 99 percent are left-handed. \n",
    "How many left-handed people have to leave the room to bring that percentage down to 98 percent?\n",
    "\"\"\"\n",
    "\n",
    "# Format message\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": no_cot}]}]\n",
    "\n",
    "# Call Nova\n",
    "model_response, content_text = call_nova(model_id, messages, max_tokens=1024)\n",
    "\n",
    "# Show response\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intentemos otro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Elige tu modelo, acorde a tu regiÃ³n: https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html \n",
    "# - Intenta con otros modelos: anthropic.claude-3-5-haiku-20241022-v1:0, anthropic.claude-3-haiku-20240307-v1:0\n",
    "anthropic_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Call Bedrock Runtime\n",
    "response = bedrock_runtime.converse(\n",
    "      modelId=anthropic_model_id,\n",
    "      system=[{\n",
    "        \"text\": \"Read the following text and answer the question.\"\n",
    "      }],\n",
    "      messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": no_cot}]\n",
    "      }],\n",
    "      inferenceConfig={\n",
    "          \"maxTokens\": 4096,\n",
    "          \"temperature\": 0.5\n",
    "      },\n",
    "    )\n",
    "\n",
    "# Print the response\n",
    "response_text = response['output']['message']['content'][0]['text']\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora usemos CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User question\n",
    "user_question = \"In a room of 100 people, 99 percent are left-handed. How many left-handed people have to leave the room to bring that percentage down to 98 percent?\"\n",
    "\n",
    "#Â Guided Chain of Thought\n",
    "guided_cot = f\"\"\"For tasks requiring reasoning or math, use the Chain-of-Thought methodology to explain your step-by-step calculations or logic\n",
    "\n",
    "Solve the questions systematically, following these guidelines:\n",
    "\n",
    "You are using a Chain of Thought (CoT) approach with reflection to answer queries. Follow these steps:\n",
    "\n",
    "1. Think through the problem step by step within the <thinking> tags.\n",
    "2. Reflect on your thinking to check for any errors or improvements within the <reflection> tags.\n",
    "3. Make any necessary adjustments based on your reflection.\n",
    "4. Show the steps in Markdown format you took to solve the problem, including reasoning, reflection and calculations.\n",
    "\n",
    "Now, apply these steps to solve:\n",
    "{user_question}\n",
    "\n",
    "Show your work at each step, explaining your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": guided_cot}]}]\n",
    "model_response, content_text = call_nova(model_id, messages, max_tokens=2048)\n",
    "\n",
    "# Print the response\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con otro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Intenta con otros modelos: anthropic.claude-3-5-haiku-20241022-v1:0, anthropic.claude-3-haiku-20240307-v1:0, us.deepseek.r1-v1:0, us.meta.llama3-2-3b-instruct-v1:0\n",
    "anthropic_model_id = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "      modelId=anthropic_model_id,\n",
    "      system=[{\n",
    "        \"text\": \"Read the following Chain-of-Thought (CoT) reasoning process and answer the question.\"\n",
    "      }],\n",
    "      messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": guided_cot}]\n",
    "      }],\n",
    "      inferenceConfig={\n",
    "          \"maxTokens\": 4096,\n",
    "          \"temperature\": 0.5\n",
    "      },\n",
    "    )\n",
    "\n",
    "# Print the response\n",
    "response_text = response['output']['message']['content'][0]['text']\n",
    "print_output(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Elige tu modelo, acorde a tu regiÃ³n: https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html \n",
    "# - Intenta con otros modelos de Amazon: us.amazon.nova-micro-v1:0, us.amazon.nova-lite-v1:0, us.amazon.nova-pro-v1:0\n",
    "amzn_model_id = \"us.amazon.nova-micro-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot = \"\"\"You are a project manager for a small software development team tasked with launching a new app feature.\n",
    "You want to streamline the development process and ensure timely delivery. Draft a project plan\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": no_cot}]}]\n",
    "\n",
    "model_response, content_text = call_nova(amzn_model_id, messages, max_tokens=1024)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_cot = \"\"\"You are a project manager for a small software development team tasked with launching a new app feature.\n",
    "You want to streamline the development process and ensure timely delivery.\n",
    "Your task is to draft a project plan.\n",
    "\n",
    "But first do some thinking on how you want to structure and go through below questions before starting the draft.\n",
    "Please follow these steps:\n",
    "1. Think about who the audience is (this is for CEOs, CTOs and other executives)\n",
    "2. Think about what to start with\n",
    "3. Think about what Challenges you want to solve with this app\n",
    "4. Think about the Tasks that will be needed to be completed\n",
    "5. Create Milestones\n",
    "6. Monitor Progress and Optimize\n",
    "Explain all your thinking in <thinking></thinking> XML Tags and then write the final copy of project plan for executives in <project_plan></project_plan> XML Tag.\n",
    "\n",
    "Output Schema:\n",
    "<thinking>\n",
    "( thoughts to above questions)\n",
    "</thinking>\n",
    "<project_plan>\n",
    "( project plan)\n",
    "</project_plan>\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": guided_cot}]}]\n",
    "\n",
    "model_response, content_text = call_nova(amzn_model_id, messages, max_tokens=2048)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirma acceso a modelos, como son Sonnet 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Intenta con otros modelos: us.anthropic.claude-3-7-sonnet-20250219-v1:0, anthropic.claude-3-5-haiku-20241022-v1:0, anthropic.claude-3-haiku-20240307-v1:0, us.deepseek.r1-v1:0, us.meta.llama3-2-3b-instruct-v1:0\n",
    "anthropic_model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "      modelId=anthropic_model_id,\n",
    "      system=[{\n",
    "        \"text\": \"Read the following Chain-of-Thought (CoT) reasoning process and answer the question.\"\n",
    "      }],\n",
    "      messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": guided_cot}]\n",
    "      }],\n",
    "      inferenceConfig={\n",
    "          \"maxTokens\": 4096,\n",
    "          \"temperature\": 0.5\n",
    "      },\n",
    "    )\n",
    "\n",
    "# Print the response\n",
    "response_text = response['output']['message']['content'][0]['text']\n",
    "print_output(response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
