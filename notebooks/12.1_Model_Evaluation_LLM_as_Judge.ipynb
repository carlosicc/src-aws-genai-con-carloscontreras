{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Model-as-a-Judge Evaluation Guide\n",
    "\n",
    "> Original source: [aws-samples-notebook](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/evaluation-observe/bedrock-llm-as-judge-evaluation/model-as-a-judge.ipynb)\n",
    "\n",
    "### See Model Evaluation support by Region \n",
    "\n",
    "Model evaluation by Region [docs](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-support.html)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This notebook demonstrates how to use Amazon Bedrock's Model-as-a-Judge feature for systematic model evaluation. The Model-as-a-Judge approach uses a foundation model to score another model's responses and provide explanations for the scores. The guide covers creating evaluation datasets, running evaluations, and comparing different foundation models.\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Dataset Generation](#dataset)\n",
    "3. [S3 Integration](#s3)\n",
    "4. [Single Model Evaluation](#single)\n",
    "5. [Model Selection and Comparison](#comparison)\n",
    "6. [Monitoring and Results](#monitoring)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- An AWS account with Bedrock access\n",
    "- Appropriate IAM roles and permissions\n",
    "- Access to supported evaluator models (Claude 3 Haiku, Claude 3.5 Sonnet, Mistral Large, or Meta Llama 3.1)\n",
    "- An S3 bucket for storing evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import random\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup <a name=\"setup\"></a>\n",
    "> Cambien a sus datos (e.g. bucket name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "REGION = \"us-west-2\"\n",
    "BUCKET_NAME = \"genai-carlos-contreras-bucket-data-quarks-labs-oregon-01\"\n",
    "PREFIX = \"labs/model-evaluation\"\n",
    "dataset_custom_name = \"fruit-discounts-data\"\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_client = boto3.client('bedrock', region_name=REGION)\n",
    "s3_client = boto3.client('s3', region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create IAM Role for this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the IAM role name\n",
    "import string\n",
    "random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=6))\n",
    "role_name = 'AdminOperBedrockFullAccess-GenAi-' + random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IAM and S3 clients\n",
    "iam = boto3.client('iam')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Check if the role already exists\n",
    "try:\n",
    "    existing_role = iam.get_role(RoleName=role_name)\n",
    "    print(f\"IAM role '{role_name}' already exists.\")\n",
    "    ROLE_ARN = existing_role['Role']['Arn']\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "        # Role doesn't exist, create it\n",
    "        trust_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                        \"Service\": \"bedrock.amazonaws.com\"\n",
    "                    },\n",
    "                    \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Create the IAM role\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "            Description='IAM role for Bedrock with full access and S3 read/write access'\n",
    "        )\n",
    "\n",
    "        # Attach the AmazonBedrockFullAccess managed policy.\n",
    "        # IMPORTANT: This grants full access to Bedrock, so only for demo purposes.\n",
    "        iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonBedrockFullAccess'\n",
    "        )\n",
    "\n",
    "        # Define the S3 access policy\n",
    "        s3_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:PutObject\",\n",
    "                        \"s3:ListBucket\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        f\"arn:aws:s3:::{BUCKET_NAME}\",\n",
    "                        f\"arn:aws:s3:::{BUCKET_NAME}/*\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Define the policy name\n",
    "        policy_name = f'{role_name}S3Policy'\n",
    "\n",
    "        # Check if the policy already exists\n",
    "        try:\n",
    "            existing_policy = iam.get_policy(PolicyArn=f'arn:aws:iam::{boto3.client(\"sts\").get_caller_identity()[\"Account\"]}:policy/{policy_name}')\n",
    "            print(f\"Policy '{policy_name}' already exists.\")\n",
    "            policy_arn = existing_policy['Policy']['Arn']\n",
    "        except iam.exceptions.NoSuchEntityException:\n",
    "            # Policy doesn't exist, create it\n",
    "            s3_policy_response = iam.create_policy(\n",
    "                PolicyName=policy_name,\n",
    "                PolicyDocument=json.dumps(s3_policy)\n",
    "            )\n",
    "            policy_arn = s3_policy_response['Policy']['Arn']\n",
    "            print(f\"Policy '{policy_name}' created successfully.\")\n",
    "\n",
    "        # Attach the S3 access policy to the role\n",
    "        iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=policy_arn\n",
    "        )\n",
    "\n",
    "        ROLE_ARN = response['Role']['Arn']\n",
    "        print(f\"IAM role '{role_name}' created successfully with AmazonBedrockFullAccess and S3 access policies attached.\")\n",
    "    else:\n",
    "        # If there's an error other than NoSuchEntity, re-raise it\n",
    "        raise\n",
    "\n",
    "print(f\"Role ARN: {ROLE_ARN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation <a name=\"dataset\"></a>\n",
    "\n",
    "We'll create a simple dataset of mathematical reasoning problems. These problems test:\n",
    "- Basic arithmetic\n",
    "- Logical reasoning\n",
    "- Natural language understanding\n",
    "\n",
    "The dataset follows the required JSONL format for Bedrock evaluation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def generate_shopping_problems(num_problems=50):\n",
    "    \"\"\"Generate shopping-related math problems with random values.\"\"\"\n",
    "    problems = []\n",
    "    items = [\"apples\", \"oranges\", \"bananas\", \"books\", \"pencils\", \"notebooks\"]\n",
    "    \n",
    "    for _ in range(num_problems):\n",
    "        # Generate random values\n",
    "        item = random.choice(items)\n",
    "        quantity = random.randint(3, 20)\n",
    "        price_per_item = round(random.uniform(1.5, 15.0), 2)\n",
    "        discount_percent = random.choice([10, 15, 20, 25, 30])\n",
    "        \n",
    "        # Calculate the answer\n",
    "        total_price = quantity * price_per_item\n",
    "        discount_amount = total_price * (discount_percent / 100)\n",
    "        final_price = round(total_price - discount_amount, 2)\n",
    "        \n",
    "        # Create the problem\n",
    "        problem = {\n",
    "            \"prompt\": f\"If {item} cost ${price_per_item} each and you buy {quantity} of them with a {discount_percent}% discount, how much will you pay in total?\",\n",
    "            \"category\": \"Shopping Math\",\n",
    "            \"referenceResponse\": f\"The total price will be ${final_price}. Original price: ${total_price} minus {discount_percent}% discount (${discount_amount})\"\n",
    "        }\n",
    "        \n",
    "        problems.append(problem)\n",
    "    \n",
    "    return problems\n",
    "\n",
    "\n",
    "def save_to_jsonl(problems, output_file):\n",
    "    \"\"\"Save the problems to a JSONL file.\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for problem in problems:\n",
    "            f.write(json.dumps(problem) + '\\n')\n",
    "\n",
    "SAMPLE_SIZE = 30\n",
    "problems = generate_shopping_problems(SAMPLE_SIZE)\n",
    "save_to_jsonl(problems, f\"evaluation/{dataset_custom_name}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Integration <a name=\"s3\"></a>\n",
    "\n",
    "After generating our sample dataset, we need to upload it to S3 for use in the evaluation job. \n",
    "We'll use the boto3 S3 client to upload our JSONL file.\n",
    "\n",
    "> **Note**: Make sure your IAM role has appropriate S3 permissions (s3:PutObject) for the target bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(local_file: str, bucket: str, s3_key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Upload a file to S3 with error handling.\n",
    "    \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(local_file, bucket, s3_key)\n",
    "        print(f\"✓ Successfully uploaded to s3://{bucket}/{s3_key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error uploading to S3: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Upload dataset\n",
    "s3_key = f\"{PREFIX}/{dataset_custom_name}.jsonl\"\n",
    "upload_success = upload_to_s3(f\"evaluation/{dataset_custom_name}.jsonl\", BUCKET_NAME, s3_key)\n",
    "\n",
    "if not upload_success:\n",
    "    raise Exception(\"Failed to upload dataset to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Job Configuration\n",
    "\n",
    "Configure the LLM-as-Judge evaluation with comprehensive metrics for assessing model performance:\n",
    "\n",
    "| Metric Category | Description |\n",
    "|----------------|-------------|\n",
    "| Quality | Correctness, Completeness, Faithfulness |\n",
    "| User Experience | Helpfulness, Coherence, Relevance |\n",
    "| Instructions | Following Instructions, Professional Style |\n",
    "| Safety | Harmfulness, Stereotyping, Refusal |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_llm_judge_evaluation(\n",
    "    client,\n",
    "    job_name: str,\n",
    "    role_arn: str,\n",
    "    input_s3_uri: str,\n",
    "    output_s3_uri: str,\n",
    "    evaluator_model_id: str,\n",
    "    generator_model_id: str,\n",
    "    dataset_name: str = None,\n",
    "    task_type: str = \"General\" # must be General for LLMaaJ\n",
    "):    \n",
    "    # All available LLM-as-judge metrics\n",
    "    llm_judge_metrics = [\n",
    "        \"Builtin.Correctness\",\n",
    "        \"Builtin.Completeness\", \n",
    "        \"Builtin.Faithfulness\",\n",
    "        \"Builtin.Helpfulness\",\n",
    "        \"Builtin.Coherence\",\n",
    "        \"Builtin.Relevance\",\n",
    "        \"Builtin.FollowingInstructions\",\n",
    "        \"Builtin.ProfessionalStyleAndTone\",\n",
    "        \"Builtin.Harmfulness\",\n",
    "        \"Builtin.Stereotyping\",\n",
    "        \"Builtin.Refusal\"\n",
    "    ]\n",
    "\n",
    "    # Configure dataset\n",
    "    dataset_config = {\n",
    "        \"name\": dataset_name or \"CustomDataset\",\n",
    "        \"datasetLocation\": {\n",
    "            \"s3Uri\": input_s3_uri\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.create_evaluation_job(\n",
    "            jobName=job_name,\n",
    "            roleArn=role_arn,\n",
    "            applicationType=\"ModelEvaluation\",\n",
    "            evaluationConfig={\n",
    "                \"automated\": {\n",
    "                    \"datasetMetricConfigs\": [\n",
    "                        {\n",
    "                            \"taskType\": task_type,\n",
    "                            \"dataset\": dataset_config,\n",
    "                            \"metricNames\": llm_judge_metrics\n",
    "                        }\n",
    "                    ],\n",
    "                    \"evaluatorModelConfig\": {\n",
    "                        \"bedrockEvaluatorModels\": [\n",
    "                            {\n",
    "                                \"modelIdentifier\": evaluator_model_id\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            inferenceConfig={\n",
    "                \"models\": [\n",
    "                    {\n",
    "                        \"bedrockModel\": {\n",
    "                            \"modelIdentifier\": generator_model_id\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            outputDataConfig={\n",
    "                \"s3Uri\": output_s3_uri\n",
    "            }\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating evaluation job: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Evaluation <a name=\"single\"></a>\n",
    "\n",
    "First, let's run a single evaluation job using Claude 3 Haiku as both generator and evaluator.\n",
    "\n",
    "### Note:\n",
    "⚠️Confirm the MODEL is supported [here](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-support.html), for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Job Configuration\n",
    "evaluator_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "generator_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Job Name\n",
    "job_name = f\"llmaaj-{generator_model.split('.')[0]}-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# S3 Paths\n",
    "input_data = f\"s3://{BUCKET_NAME}/{PREFIX}/{dataset_custom_name}.jsonl\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/{PREFIX}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation job\n",
    "try:\n",
    "    llm_as_judge_response = create_llm_judge_evaluation(\n",
    "        client=bedrock_client,\n",
    "        job_name=job_name,\n",
    "        role_arn=ROLE_ARN,\n",
    "        input_s3_uri=input_data,\n",
    "        output_s3_uri=output_path,\n",
    "        evaluator_model_id=evaluator_model,\n",
    "        generator_model_id=generator_model,\n",
    "        task_type=\"General\"\n",
    "    )\n",
    "    print(f\"✓ Created evaluation job: {llm_as_judge_response['jobArn']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to create evaluation job: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Job Progress\n",
    "Track the status of your evaluation job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get job ARN based on job type\n",
    "evaluation_job_arn = llm_as_judge_response['jobArn']\n",
    "\n",
    "# Check job status\n",
    "check_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn) \n",
    "print(f\"Job Status: {check_status['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Comparison <a name=\"comparison\"></a>\n",
    "\n",
    "Now, let's evaluate multiple generator models to find the optimal model for our use case. We'll compare different foundation models while using a consistent evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANTE 🚨\n",
    "> Confirma permisos de cuenta y región sobre el modelo a usar en Evaluación:  (Bedrock -> Model Access)\n",
    "- Por ejemplo, permisos sobre DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GENERATOR_MODELS = [\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "# Consistent Evaluator\n",
    "EVALUATOR_MODEL = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "def run_model_comparison(\n",
    "    generator_models: List[str],\n",
    "    evaluator_model: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    evaluation_jobs = []\n",
    "    \n",
    "    for generator_model in generator_models:\n",
    "        job_string = generator_model.replace('.', '-').replace(':', '-')\n",
    "        job_name = f\"llmaaj-{job_string}-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "        \n",
    "        try:\n",
    "            response = create_llm_judge_evaluation(\n",
    "                client=bedrock_client,\n",
    "                job_name=job_name,\n",
    "                role_arn=ROLE_ARN,\n",
    "                input_s3_uri=input_data,\n",
    "                output_s3_uri=f\"{output_path}/{job_name}/\",\n",
    "                evaluator_model_id=evaluator_model,\n",
    "                generator_model_id=generator_model,\n",
    "                task_type=\"General\"\n",
    "            )\n",
    "            \n",
    "            job_info = {\n",
    "                \"job_name\": job_name,\n",
    "                \"job_arn\": response[\"jobArn\"],\n",
    "                \"generator_model\": generator_model,\n",
    "                \"evaluator_model\": evaluator_model,\n",
    "                \"status\": \"CREATED\"\n",
    "            }\n",
    "            evaluation_jobs.append(job_info)\n",
    "            \n",
    "            print(f\"✓ Created job: {job_name}\")\n",
    "            print(f\"  Generator: {generator_model}\")\n",
    "            print(f\"  Evaluator: {evaluator_model}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with {generator_model}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return evaluation_jobs\n",
    "\n",
    "# Run model comparison\n",
    "evaluation_jobs = run_model_comparison(GENERATOR_MODELS, EVALUATOR_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Results <a name=\"monitoring\"></a>\n",
    "\n",
    "Track the progress of all evaluation jobs and display their current status.\n",
    "\n",
    "> Note: at the moment, Jan 2025, this process takes around 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to check job status\n",
    "def check_jobs_status(jobs, client):\n",
    "    \"\"\"Check and update status for all evaluation jobs\"\"\"\n",
    "    for job in jobs:\n",
    "        try:\n",
    "            response = client.get_evaluation_job(\n",
    "                jobIdentifier=job[\"job_arn\"]\n",
    "            )\n",
    "            job[\"status\"] = response[\"status\"]\n",
    "        except Exception as e:\n",
    "            job[\"status\"] = f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "# Check initial status\n",
    "updated_jobs = check_jobs_status(evaluation_jobs, bedrock_client)\n",
    "\n",
    "# Display status summary\n",
    "for job in updated_jobs:\n",
    "    print(f\"Job: {job['job_name']}\")\n",
    "    print(f\"Status: {job['status']}\")\n",
    "    print(f\"Generator: {job['generator_model']}\")\n",
    "    print(f\"Evaluator: {job['evaluator_model']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Recuerda borrar las políticas IAM creadas por este notebook. \n",
    "- Tip: Puedes usar el notebook ```Clean_up_Resources_eg_iam_policies.ipynb``` para esto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
