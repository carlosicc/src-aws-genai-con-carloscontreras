{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardrails en Amazon Bedrock\n",
    "> Notebook original: [aws-bedrock-samples](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/responsible_ai/bedrock-guardrails/Apply_Guardrail_with_Streaming_and_Long_Context.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from loguru import logger\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Checa las regiones soportadas de Guardrails [aquí](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-supported.html)\n",
    "\n",
    "Nota:\n",
    "La API ApplyGuardrail tiene un límite predeterminado de 25 unidades de texto (aproximadamente 25,000 caracteres) por segundo. Si la entrada excede este límite, debe dividirse en trozos y procesarse secuencialmente para evitar throttling.\n",
    "\n",
    "Más en [aws-blogs](https://aws.amazon.com/blogs/machine-learning/use-the-applyguardrail-api-with-long-context-inputs-and-streaming-outputs-in-amazon-bedrock/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's specify the parameters needed for execution later\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "TEXT_UNIT = 1000 # characters\n",
    "LIMIT_TEXT_UNIT = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have AWS credentials or AWS profile setup before running this cell\n",
    "bedrock_client = boto3.client(\"bedrock\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {\n",
    "                'name': 'Fiduciary Advice',\n",
    "                'definition': 'Providing personalized advice or recommendations on managing financial assets, investments, or trusts in a fiduciary capacity or assuming related obligations and liabilities.',\n",
    "                'examples': [\n",
    "                    'What stocks should I invest in for my retirement?',\n",
    "                    'Is it a good idea to put my money in a mutual fund?',\n",
    "                    'How should I allocate my 401(k) investments?',\n",
    "                    'What type of trust fund should I set up for my children?',\n",
    "                    'Should I hire a financial advisor to manage my investments?'\n",
    "                ],\n",
    "                'type': 'DENY'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "contentPolicyConfig={\n",
    "    'filtersConfig': [\n",
    "        {\n",
    "            'type': 'SEXUAL',\n",
    "            'inputStrength': 'HIGH',\n",
    "            'outputStrength': 'HIGH'\n",
    "        },\n",
    "        {\n",
    "            'type': 'VIOLENCE',\n",
    "            'inputStrength': 'HIGH',\n",
    "            'outputStrength': 'HIGH'\n",
    "        },\n",
    "        {\n",
    "            'type': 'HATE',\n",
    "            'inputStrength': 'HIGH',\n",
    "            'outputStrength': 'HIGH'\n",
    "        },\n",
    "        {\n",
    "            'type': 'INSULTS',\n",
    "            'inputStrength': 'HIGH',\n",
    "            'outputStrength': 'HIGH'\n",
    "        },\n",
    "        {\n",
    "            'type': 'MISCONDUCT',\n",
    "            'inputStrength': 'HIGH',\n",
    "            'outputStrength': 'HIGH'\n",
    "        },\n",
    "        {\n",
    "            'type': 'PROMPT_ATTACK',\n",
    "            'inputStrength': 'HIGH',\n",
    "            'outputStrength': 'NONE'\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "wordPolicyConfig={\n",
    "    'wordsConfig': [\n",
    "        {\n",
    "            'text': 'fiduciary advice'\n",
    "        },\n",
    "        {\n",
    "            'text': 'investment recommendations'\n",
    "        },\n",
    "        {\n",
    "            'text': 'stock picks'\n",
    "        },\n",
    "        {\n",
    "            'text': 'financial planning guidance'\n",
    "        },\n",
    "        {\n",
    "            'text': 'portfolio allocation advice'\n",
    "        },\n",
    "        {\n",
    "            'text': 'retirement fund suggestions'\n",
    "        },\n",
    "        {\n",
    "            'text': 'wealth management tips'\n",
    "        },\n",
    "        {\n",
    "            'text': 'trust fund setup'\n",
    "        },\n",
    "        {\n",
    "            'text': 'investment strategy'\n",
    "        },\n",
    "        {\n",
    "            'text': 'financial advisor recommendations'\n",
    "        }\n",
    "    ],\n",
    "    'managedWordListsConfig': [\n",
    "        {\n",
    "            'type': 'PROFANITY'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sensitiveInformationPolicyConfig={\n",
    "    'piiEntitiesConfig': [\n",
    "        {\n",
    "            'type': 'EMAIL',\n",
    "            'action': 'ANONYMIZE'\n",
    "        },\n",
    "        {\n",
    "            'type': 'PHONE',\n",
    "            'action': 'ANONYMIZE'\n",
    "        },\n",
    "        {\n",
    "            'type': 'NAME',\n",
    "            'action': 'ANONYMIZE'\n",
    "        },\n",
    "        {\n",
    "            'type': 'US_SOCIAL_SECURITY_NUMBER',\n",
    "            'action': 'BLOCK'\n",
    "        },\n",
    "        {\n",
    "            'type': 'US_BANK_ACCOUNT_NUMBER',\n",
    "            'action': 'BLOCK'\n",
    "        },\n",
    "        {\n",
    "            'type': 'CREDIT_DEBIT_CARD_NUMBER',\n",
    "            'action': 'BLOCK'\n",
    "        }\n",
    "    ],\n",
    "    'regexesConfig': [\n",
    "        {\n",
    "            'name': 'Account Number',\n",
    "            'description': 'Matches account numbers in the format XXXXXX1234',\n",
    "            'pattern': r'\\b\\d{6}\\d{4}\\b',\n",
    "            'action': 'ANONYMIZE'\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockedInputMessaging = \"\"\"I apologize, but I am not able to provide fiduciary advice. \n",
    "Additionally, it seems that you may have included some sensitive personal or financial information in your request. \n",
    "For your privacy and security, please modify your input and try again without including any personal, \n",
    "financial, or restricted details.\n",
    "\"\"\"\n",
    "\n",
    "blockedOutputsMessaging=\"\"\"I apologize, but I am not able to provide fiduciary advice. \n",
    "Additionally, it seems that you may have included some sensitive personal or financial information in your request. \n",
    "For your privacy and security, please modify your input and try again without including any personal, \n",
    "financial, or restricted details.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    response = bedrock_client.create_guardrail(\n",
    "        name='fiduciary-advice-advanced',\n",
    "        description='Prevents the our model from providing fiduciary advice.',\n",
    "        topicPolicyConfig=topicPolicyConfig,\n",
    "        contentPolicyConfig=contentPolicyConfig,\n",
    "        wordPolicyConfig=wordPolicyConfig,\n",
    "        sensitiveInformationPolicyConfig=sensitiveInformationPolicyConfig,\n",
    "        blockedInputMessaging=blockedInputMessaging,\n",
    "        blockedOutputsMessaging=blockedOutputsMessaging,\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConflictException':\n",
    "        # Guardrail already exists, retrieve the existing guardrail information\n",
    "        list_response = bedrock_client.list_guardrails()\n",
    "        for guardrail in list_response['guardrails']:\n",
    "            if guardrail['name'] == 'fiduciary-advice':\n",
    "                logger.warning('Guardrail already exists.')\n",
    "                pprint.pprint(guardrail)\n",
    "                \n",
    "                # Get values for guardrail_id and guardrail_version\n",
    "                guardrail_id = guardrail['id']\n",
    "                guardrail_version = guardrail['version']\n",
    "    else:\n",
    "        # Handle other exceptions\n",
    "        raise e\n",
    "else:\n",
    "    # Guardrail creation was successful\n",
    "    print(f\"Guardrail created: {response}\")\n",
    "    guardrail_id = response['guardrailId']\n",
    "    guardrail_version = response['version'] \n",
    "\n",
    "    print(f\"Guardrail ID: {response['guardrailId']}\")\n",
    "    print(f\"Guardrail Version: {response['version'] }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso\n",
    "La solicitud ApplyGuardrail permite al cliente pasar todo su contenido que debe protegerse utilizando su Guardrail definido. El campo de origen debe establecerse en INPUT cuando el contenido a evaluar proviene de un usuario, generalmente el aviso del LLM. La fuente debe establecerse en OUTPUT cuando se debe hacer cumplir el Guardrail de salida del modelo, generalmente una respuesta de LLM.\n",
    "\n",
    "#### Estrategia\n",
    "Mostraremos cómo puede aplicar el guardrail en múltiples escenarios:\n",
    "\n",
    "1. Contenido de entrada pequeño (<25 unidades de texto)\n",
    "2. Contenido de entrada grande (>25 unidades de texto)\n",
    "3. Salida de LLM en streaming\n",
    "\n",
    "Si el contenido es mayor que los límites de cuota de la API ApplyGuardrail, tendremos que dividir el contenido original en trozos más pequeños para no alcanzar el límite de throttling.\n",
    "\n",
    "Además, en el caso del streaming, los trozos podrían contener solo unos pocos tokens, no sería sabio aplicar el guardrail en cada nuevo trozo, ni sería factible esperar a que se genere toda la salida para luego aplicar el guardrail. En su lugar, para encontrar el mejor ajuste, podemos aplicar el guardrail cada vez que tengamos suficientes tokens, es decir, tokens ~= 1 unidad de texto, esto asegurará tanto el control de costos como la cantidad suficiente de contenido disponible en el trozo para encontrar contenido que potencialmente viole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_severe_violations(violations):\n",
    "    # When guardrail intervenes either the action on the request is BLOCKED or NONE\n",
    "    # Here we check how many of the violations lead to blocking the request\n",
    "    severe_violations = [violation['action']=='BLOCKED' for violation in violations]\n",
    "    return sum(severe_violations)\n",
    "\n",
    "def is_policy_assessement_blocked(assessments):\n",
    "    # While creating the guardrail you could specify multiple types of policies.\n",
    "    # At the time of assessment all the policies should be checked for potential violations\n",
    "    # If there is even 1 violation that blocks the request, the entire request is blocked\n",
    "    blocked = []\n",
    "    for assessment in assessments:\n",
    "        if 'topicPolicy' in assessment:\n",
    "            blocked.append(check_severe_violations(assessment['topicPolicy']['topics']))\n",
    "        if 'wordPolicy' in assessment:\n",
    "            if 'customWords' in assessment['wordPolicy']:\n",
    "                blocked.append(check_severe_violations(assessment['wordPolicy']['customWords']))\n",
    "            if 'managedWordLists' in assessment['wordPolicy']:\n",
    "                blocked.append(check_severe_violations(assessment['wordPolicy']['managedWordLists']))\n",
    "        if 'sensitiveInformationPolicy' in assessment:\n",
    "            if 'piiEntities' in assessment['sensitiveInformationPolicy']:\n",
    "                blocked.append(check_severe_violations(assessment['sensitiveInformationPolicy']['piiEntities']))\n",
    "            if 'regexes' in assessment['sensitiveInformationPolicy']:\n",
    "                blocked.append(check_severe_violations(assessment['sensitiveInformationPolicy']['regexes']))\n",
    "        if 'contentPolicy' in assessment:\n",
    "            blocked.append(check_severe_violations(assessment['contentPolicy']['filters']))\n",
    "    severe_violation_count = sum(blocked)\n",
    "    logger.error(f\"::Guardrail:: {severe_violation_count} severe violations detected\")\n",
    "    \n",
    "    return severe_violation_count>0\n",
    "\n",
    "\n",
    "def apply_guardrail(text, text_source_type, guardrail_id, guardrail_version=\"DRAFT\"):\n",
    "    logger.success(f\"::Guardrail:: Applying guardrail with {(len(text)//TEXT_UNIT)+1} text units\")\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version, \n",
    "        source=text_source_type, # can be 'INPUT' or 'OUTPUT'\n",
    "        content=[{\"text\": {\"text\": text}}]\n",
    "    )\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        is_blocked = is_policy_assessement_blocked(response['assessments'])\n",
    "        alternate_text = ' '.join([output['text'] for output in response['outputs']])\n",
    "        return is_blocked, alternate_text, response\n",
    "    else:\n",
    "        # Return the default response in case of no guardrail intervention\n",
    "        return False, text, response\n",
    "\n",
    "\n",
    "def apply_guardrail_full_text(text, text_source_type, guardrail_id, guardrail_version=\"DRAFT\"):\n",
    "    text_length = len(text)\n",
    "    filtered_text = ''\n",
    "    if text_length <= LIMIT_TEXT_UNIT*TEXT_UNIT:\n",
    "        return apply_guardrail(text, text_source_type, guardrail_id, guardrail_version)\n",
    "    else:\n",
    "        # If the text length is greater than the default text unit limits then it's better to chunk the text to avoid throttling.\n",
    "        for i, chunk in enumerate(wrap(text, LIMIT_TEXT_UNIT*TEXT_UNIT)):\n",
    "            print(f'::Guardrail::Applying guardrails at chunk {i+1}')\n",
    "            is_blocked, alternate_text, response = apply_guardrail(chunk, text_source_type, guardrail_id, guardrail_version)\n",
    "            if is_blocked:\n",
    "                filtered_text = alternate_text\n",
    "                break\n",
    "            # It could be the case that guardrails intervened and anonymized PII in the input text,\n",
    "            # we can then take the output from guardrails to create filtered text response.\n",
    "            filtered_text += alternate_text\n",
    "        return is_blocked, filtered_text, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def stream_conversation(messages,\n",
    "                        system_prompts,\n",
    "                        inference_config,\n",
    "                        additional_model_fields):\n",
    "    \n",
    "    response = bedrock_runtime.converse_stream(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    stream = response.get('stream')\n",
    "    full_text = \"\"\n",
    "    buffer_text = \"\"\n",
    "    applied_guardrails = []\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                logger.info(f\"Role: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                new_text = event['contentBlockDelta']['delta']['text']\n",
    "\n",
    "                if len(buffer_text + new_text) > TEXT_UNIT:\n",
    "                    is_blocked, alt_text, guardrail_response = apply_guardrail(buffer_text, \"OUTPUT\", guardrail_id, guardrail_version)\n",
    "                    if is_blocked:\n",
    "                        event['messageStop'] = {\n",
    "                            'stopReason': guardrail_response['action'], \n",
    "                            'output': alt_text,\n",
    "                            'assessments': guardrail_response['assessments'],\n",
    "                        }\n",
    "                        full_text = alt_text\n",
    "                    else:\n",
    "                        full_text += alt_text\n",
    "                    print(alt_text, end=\"\")\n",
    "                    applied_guardrails.append(guardrail_response)\n",
    "                    buffer_text = new_text\n",
    "                else: \n",
    "                    buffer_text += new_text\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                if event['messageStop']['stopReason'] == 'GUARDRAIL_INTERVENED':\n",
    "                    logger.warning(f\"Stop reason: {event['messageStop']['stopReason']}\")\n",
    "                    break\n",
    "                else:\n",
    "                    logger.warning(f\"Stop reason: {event['messageStop']['stopReason']}\")\n",
    "                    is_blocked, alt_text, guardrail_response = apply_guardrail(buffer_text, \"OUTPUT\", guardrail_id, guardrail_version)\n",
    "                    if is_blocked:\n",
    "                        print(alt_text)\n",
    "                        if 'metadata' not in event:\n",
    "                            event['metadata'] = {}\n",
    "                        event['metadata']['guardrails_usage'] = guardrail_response['usage']\n",
    "                        applied_guardrails.append(guardrail_response)\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    logger.info(\"Token usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                    print(f\":Total text units: {(len(full_text)//TEXT_UNIT)+1}\")\n",
    "                if 'metrics' in event['metadata']:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "                if 'guardrails_usage' in event['metadata']:\n",
    "                    print(event['metadata']['guardrails_usage'])\n",
    "    return full_text, applied_guardrails\n",
    "\n",
    "\n",
    "def generate(input_message):\n",
    "\n",
    "    system_prompt = \"\"\"You are an assistant that helps with tasks from users. Be as elaborate as possible\"\"\"\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_message}]\n",
    "    }\n",
    "    messages = [message]\n",
    "    \n",
    "    # System prompts.\n",
    "    system_prompts = [{\"text\" : system_prompt}]\n",
    "\n",
    "    # inference parameters to use.\n",
    "    temperature = 0.5\n",
    "\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    # Additional model inference parameters.\n",
    "    additional_model_fields = {}\n",
    "\n",
    "    try:\n",
    "        full_text, applied_guardrails = stream_conversation(messages, system_prompts, inference_config, additional_model_fields)\n",
    "    except ClientError as err:\n",
    "        message = err.response['Error']['Message']\n",
    "        logger.error(\"A client error occured: {message}\")\n",
    "\n",
    "    else:\n",
    "        logger.info(f\"Finished streaming messages with model {MODEL_ID}.\")\n",
    "        \n",
    "    return full_text, applied_guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = \"List 3 names of prominent CEOs and later tell me what is a bank and what are the benefits of opening a savings account?\"\n",
    "full_text, applied_guardrails = generate(sample_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver claramente que el guardrail intervino anteriormente y anonimizó algunos nombres en la generación de texto. Examinemos qué evaluaciones realizó el guardrail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty Print\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardrail Assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for guardrail in applied_guardrails:\n",
    "    if guardrail['action']!='NONE':\n",
    "        pp.pprint(guardrail['assessments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior muestra que se invocó sensitiveInformationPolicy, anonimizando los nombres generados por el modelo y limpiando la salida.\n",
    "\n",
    "Ahora, podemos probar un escenario diferente donde la entrada contiene una solicitud de asesoramiento fiduciario y podemos observar la aplicación del guardrail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2 = \"Tell me about why financial independence is important and only at the very end ask the question if you can help me to invest after retirement?\"\n",
    "full_text, applied_guardrails = generate(sample_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar arriba que ocurrió la intervención del guardrail, ahora examinemos qué políticas se violaron. Para eso, podemos examinar evaluaciones, que es parte de la respuesta de la API ApplyGuardrail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for guardrail in applied_guardrails:\n",
    "    if guardrail['action']!='NONE':\n",
    "        pp.pprint(guardrail['assessments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT - Documento Pequeño\n",
    "Ahora podemos probar cómo se puede aplicar el guardrail a un documento pequeño. Utilizaremos la Carta a los accionistas de Amazon 2023, este documento no incluye ningún texto que deba hacer que el guardrail intervenga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = open('./data/guardrails/shareholder_letter.txt', 'r').read()\n",
    "print(f\"Length of the document: {len(letter)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked, new_text, guardrail_response = apply_guardrail_full_text(letter, \"INPUT\", guardrail_id, guardrail_version)\n",
    "print(f\"\\nBlocked by guardrail: {'Yes' if blocked else 'No'}\")\n",
    "if blocked:\n",
    "    print(f'Guardrail Output: {new_text}')\n",
    "elif guardrail_response=='GUARDRAIL_INTERVENED' and not blocked:\n",
    "    print(f'Filtered Text Snippet: {new_text[:5000]}')\n",
    "\n",
    "pp.pprint(guardrail_response['assessments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT - Documento Grande: \n",
    "Ahora podemos probar con un documento diferente que contiene una historia financiera ficticia generada con la ayuda de un LLM. Para aumentar la longitud del documento, podemos combinar la carta a los accionistas y la historia financiera. Esto permitirá mostrar la capacidad de dividir el documento en trozos y luego aplicar el guardrail a cada trozo individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_story = open('./data/guardrails/financial_story.txt', 'r').read()\n",
    "large_text = letter + financial_story\n",
    "print(f\"Length of the document: {len(large_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked, new_text, guardrail_response = apply_guardrail_full_text(large_text, \"INPUT\", guardrail_id, guardrail_version)\n",
    "print(f\"\\nBlocked by guardrail: {'Yes' if blocked else 'No'}\")\n",
    "if blocked:\n",
    "    print(f'Guardrail Output: {new_text}')\n",
    "elif guardrail_response=='GUARDRAIL_INTERVENED' and not blocked:\n",
    "    print(f'Filtered Text Snippet: {new_text[:5000]}')\n",
    "\n",
    "pp.pprint(guardrail_response['assessments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# No olvides borrar los Guardrails!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_client.delete_guardrail(\n",
    "    guardrailIdentifier=guardrail_id\n",
    ")\n",
    "print(response['ResponseMetadata']['HTTPStatusCode'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
