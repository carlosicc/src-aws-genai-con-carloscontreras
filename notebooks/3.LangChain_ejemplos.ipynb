{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify here your MODEL; \n",
    "# - e.g. anthropic.claude-3-5-sonnet-20240620-v1:0, meta.llama3-8b-instruct-v1:0, anthropic.claude-3-5-haiku-20241022-v1:0\n",
    "MODEL_ID=\"meta.llama3-8b-instruct-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencias:\n",
    "> Confirma que hayas instalado los requerimientos.txt, incluyendo langchain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL_ID,\n",
    "    model_kwargs=dict(temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo invocar un LLM, a través de LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Eres un asistente que traduce de Inglés a Español. Traduce el texto del usuario, por favor.\",\n",
    "    ),\n",
    "    (\"human\", \"I love this Generative AI training.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)\n",
    "> Source [here](https://python.langchain.com/docs/concepts/lcel/)\n",
    "\n",
    "> Cheatsheets [here](https://python.langchain.com/docs/how_to/lcel_cheatsheet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "runnable = RunnableLambda(lambda x: f\"Hola, {str(x)}!\")\n",
    "runnable.invoke('mundo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch a runnable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "runnable = RunnableLambda(lambda x: f\"Me gusta la serie {str(x)}!\")\n",
    "runnable.batch(['Big Bang Theory','Friends','Black Mirror'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 4)\n",
    "\n",
    "chain = runnable1 | runnable2\n",
    "\n",
    "chain.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke runnables in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 5)\n",
    "\n",
    "chain = RunnableParallel(first=runnable1, second=runnable2)\n",
    "\n",
    "chain.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn any function into a runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def func(x):\n",
    "    return x + 5\n",
    "\n",
    "runnable = RunnableLambda(func)\n",
    "runnable.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge input and output dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "runnable1 = RunnableLambda(lambda x: x[\"foo\"] + 7)\n",
    "\n",
    "chain = RunnablePassthrough.assign(bar=runnable1)\n",
    "\n",
    "chain.invoke({\"foo\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnables y Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# Function to create the system prompt\n",
    "def create_system_prompt(_):  # Add an underscore to accept and ignore the input\n",
    "    return \"You are a helpful assistant.\"\n",
    "\n",
    "# Function to create the user prompt\n",
    "def create_user_prompt(input_dict):\n",
    "    return f\"Below you have a series of tasks: \\n{input_dict['tasks']}\"\n",
    "\n",
    "# Create RunnableLambda for each prompt\n",
    "system_prompt_runnable = RunnableLambda(create_system_prompt)\n",
    "user_prompt_runnable = RunnableLambda(create_user_prompt)\n",
    "\n",
    "# Create the chain\n",
    "chain = RunnablePassthrough.assign(\n",
    "    system_prompt=system_prompt_runnable,\n",
    "    user_prompt=user_prompt_runnable\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"tasks\": \"Task 1: Escribe correo. \\nTask 2: Adjunta factura. \\nTask 3: Envía correo\"})\n",
    "\n",
    "# Remove the key ´tasks´ from dict, as RunnablePassthrough pass the input as is initially\n",
    "result.pop(\"tasks\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "> Source [here](https://python.langchain.com/docs/concepts/prompt_templates/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains y Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
    "\n",
    "chain = runnable1 | runnable2\n",
    "\n",
    "chain.invoke(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Eres un asistente que traduce de {input_language} a {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"Inglés\",\n",
    "        \"output_language\": \"Español\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL_ID,\n",
    "    model_kwargs=dict(temperature=0)\n",
    ")\n",
    "\n",
    "# Create prompt templates for each translation step\n",
    "spanish_to_german_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant that translates from Spanish to German. Please translate the user's text.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "german_to_english_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant that translates from German to English. Please translate the user's text.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = (\n",
    "    spanish_to_german_prompt \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | german_to_english_prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "spanish_input = \"Me encanta este entrenamiento de IA Generativa.\"\n",
    "result = chain.invoke({\"input\": spanish_input})\n",
    "\n",
    "print(f\"Spanish input: {spanish_input}\")\n",
    "print(f\"English output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL_ID,\n",
    "    model_kwargs=dict(temperature=0)\n",
    ")\n",
    "\n",
    "# Create prompt templates for each translation step\n",
    "spanish_to_german_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce del Español a Alemán. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "german_to_english_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Alemán a Inglés. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{german_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "def translate_spanish_to_german(inputs):\n",
    "    prompt = spanish_to_german_prompt.format_messages(input=inputs[\"input\"])\n",
    "    german_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": german_text}\n",
    "\n",
    "def translate_german_to_english(inputs):\n",
    "    prompt = german_to_english_prompt.format_messages(german_text=inputs[\"german_text\"])\n",
    "    english_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": english_text}\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    first=RunnableLambda(translate_spanish_to_german),\n",
    "    last=RunnableLambda(translate_german_to_english)\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "spanish_input = \"Me encanta este entrenamiento de IA Generativa.\"\n",
    "result = chain.invoke({\"input\": spanish_input})\n",
    "\n",
    "print(f\"Spanish input: {result['input']}\")\n",
    "print(f\"German intermediate: {result['german_text']}\")\n",
    "print(f\"English output: {result['english_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL_ID,\n",
    "    model_kwargs=dict(temperature=0)\n",
    ")\n",
    "\n",
    "# Create prompt templates for each translation step\n",
    "spanish_to_german_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce del Español a Alemán. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "german_to_english_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Alemán a Inglés. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{german_text}\")\n",
    "])\n",
    "\n",
    "english_to_mandarin_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Inglés a Chino Mandarín. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{english_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "def translate_spanish_to_german(inputs):\n",
    "    prompt = spanish_to_german_prompt.format_messages(input=inputs[\"input\"])\n",
    "    german_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": german_text}\n",
    "\n",
    "def translate_german_to_english(inputs):\n",
    "    prompt = german_to_english_prompt.format_messages(german_text=inputs[\"german_text\"])\n",
    "    english_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": english_text}\n",
    "\n",
    "def translate_english_to_mandarin(inputs):\n",
    "    prompt = english_to_mandarin_prompt.format_messages(english_text=inputs[\"english_text\"])\n",
    "    mandarin_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": inputs[\"english_text\"], \"mandarin_text\": mandarin_text}\n",
    "\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    first=RunnableLambda(translate_spanish_to_german),\n",
    "    middle=[RunnableLambda(translate_german_to_english)], \n",
    "    last=RunnableLambda(translate_english_to_mandarin)\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "spanish_input = \"Me encanta este entrenamiento de IA Generativa.\"\n",
    "result = chain.invoke({\"input\": spanish_input})\n",
    "\n",
    "print(f\"Spanish input: {result['input']}\")\n",
    "print(f\"German intermediate: {result['german_text']}\")\n",
    "print(f\"English intermediate: {result['english_text']}\")\n",
    "print(f\"Mandarin output: {result['mandarin_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL_ID,\n",
    "    model_kwargs=dict(temperature=0)\n",
    ")\n",
    "\n",
    "# Create prompt templates for each translation step\n",
    "spanish_to_german_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce del Español a Alemán. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "german_to_english_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Alemán a Inglés. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{german_text}\")\n",
    "])\n",
    "\n",
    "english_to_mandarin_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Inglés a Chino Mandarín. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{english_text}\")\n",
    "])\n",
    "\n",
    "mandarin_to_portuguese_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Chino Mandarín a Portugués. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{mandarin_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "def translate_spanish_to_german(inputs):\n",
    "    prompt = spanish_to_german_prompt.format_messages(input=inputs[\"input\"])\n",
    "    german_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": german_text}\n",
    "\n",
    "def translate_german_to_english(inputs):\n",
    "    prompt = german_to_english_prompt.format_messages(german_text=inputs[\"german_text\"])\n",
    "    english_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": english_text}\n",
    "\n",
    "def translate_english_to_mandarin(inputs):\n",
    "    prompt = english_to_mandarin_prompt.format_messages(english_text=inputs[\"english_text\"])\n",
    "    mandarin_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": inputs[\"english_text\"], \"mandarin_text\": mandarin_text}\n",
    "\n",
    "def translate_mandarin_to_portuguese(inputs):\n",
    "    prompt = mandarin_to_portuguese_prompt.format_messages(mandarin_text=inputs[\"mandarin_text\"])\n",
    "    portuguese_text = llm.invoke(prompt).content\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": inputs[\"english_text\"], \"mandarin_text\": inputs[\"mandarin_text\"], \"portuguese_text\": portuguese_text}\n",
    "\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    first=RunnableLambda(translate_spanish_to_german),\n",
    "    middle=[\n",
    "        RunnableLambda(translate_german_to_english),\n",
    "        RunnableLambda(translate_english_to_mandarin)\n",
    "        ], \n",
    "    last=RunnableLambda(translate_mandarin_to_portuguese)\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "spanish_input = \"Me encanta este entrenamiento de IA Generativa.\"\n",
    "result = chain.invoke({\"input\": spanish_input})\n",
    "\n",
    "print(f\"Spanish input: {result['input']}\")\n",
    "print(f\"German intermediate: {result['german_text']}\")\n",
    "print(f\"English intermediate: {result['english_text']}\")\n",
    "print(f\"Mandarin output: {result['mandarin_text']}\")\n",
    "print(f\"Portuguese output: {result['portuguese_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL_ID,\n",
    "    model_kwargs=dict(temperature=0)\n",
    ")\n",
    "\n",
    "# Create prompt templates for each translation step\n",
    "spanish_to_german_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce del Español a Alemán. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "german_to_english_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Alemán a Inglés. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{german_text}\")\n",
    "])\n",
    "\n",
    "english_to_mandarin_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Inglés a Chino Mandarín. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{english_text}\")\n",
    "])\n",
    "\n",
    "mandarin_to_portuguese_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente que traduce de Chino Mandarín a Portugués. Por favor, traduce el texto del usuario.\"),\n",
    "    (\"human\", \"{mandarin_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "def translate_spanish_to_german(inputs):\n",
    "    print(f\"Recibiendo input en español: '{inputs['input']}'\\n\")\n",
    "    print(\"Traduciendo de español a alemán...\")\n",
    "    prompt = spanish_to_german_prompt.format_messages(input=inputs[\"input\"])\n",
    "    german_text = llm.invoke(prompt).content\n",
    "    print(f\"Traducción al alemán completada: '{german_text}'\\n\")\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": german_text}\n",
    "\n",
    "def translate_german_to_english(inputs):\n",
    "    print(f\"Recibiendo texto en alemán: '{inputs['german_text']}'\")\n",
    "    print(\"Traduciendo de alemán a inglés...\")\n",
    "    prompt = german_to_english_prompt.format_messages(german_text=inputs[\"german_text\"])\n",
    "    english_text = llm.invoke(prompt).content\n",
    "    print(f\"Traducción al inglés completada: '{english_text}'\\n\")\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": english_text}\n",
    "\n",
    "def translate_english_to_mandarin(inputs):\n",
    "    print(f\"Recibiendo texto en inglés: '{inputs['english_text']}'\")\n",
    "    print(\"Traduciendo de inglés a mandarín...\")\n",
    "    prompt = english_to_mandarin_prompt.format_messages(english_text=inputs[\"english_text\"])\n",
    "    mandarin_text = llm.invoke(prompt).content\n",
    "    print(f\"Traducción al mandarín completada: '{mandarin_text}'\\n\")\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": inputs[\"english_text\"], \"mandarin_text\": mandarin_text}\n",
    "\n",
    "def translate_mandarin_to_portuguese(inputs):\n",
    "    print(f\"Recibiendo texto en mandarín: '{inputs['mandarin_text']}'\")\n",
    "    print(\"Traduciendo de mandarín a portugués...\")\n",
    "    prompt = mandarin_to_portuguese_prompt.format_messages(mandarin_text=inputs[\"mandarin_text\"])\n",
    "    portuguese_text = llm.invoke(prompt).content\n",
    "    print(f\"Traducción al portugués completada: '{portuguese_text}'\\n\")\n",
    "    return {\"input\": inputs[\"input\"], \"german_text\": inputs[\"german_text\"], \"english_text\": inputs[\"english_text\"], \"mandarin_text\": inputs[\"mandarin_text\"], \"portuguese_text\": portuguese_text}\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    first=RunnableLambda(translate_spanish_to_german),\n",
    "    middle=[\n",
    "        RunnableLambda(translate_german_to_english),\n",
    "        RunnableLambda(translate_english_to_mandarin)\n",
    "    ], \n",
    "    last=RunnableLambda(translate_mandarin_to_portuguese)\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "spanish_input = \"Me encanta este entrenamiento de IA Generativa.\"\n",
    "print(\"Iniciando la cadena de traducción...\")\n",
    "result = chain.invoke({\"input\": spanish_input})\n",
    "\n",
    "print(\"\\nResultados finales:\")\n",
    "print(f\"Spanish input: {result['input']}\")\n",
    "print(f\"German intermediate: {result['german_text']}\")\n",
    "print(f\"English intermediate: {result['english_text']}\")\n",
    "print(f\"Mandarin output: {result['mandarin_text']}\")\n",
    "print(f\"Portuguese output: {result['portuguese_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
