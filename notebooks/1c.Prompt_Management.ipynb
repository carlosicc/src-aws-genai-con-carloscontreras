{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Prompt Management and Flows for Amazon Bedrock\n",
    "\n",
    "> Notebook [original](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/agents-and-function-calling/bedrock-flows/Getting_started_with_Prompt_Management_Flows.ipynb)\n",
    "\n",
    "This example shows you how to get started with Prompt Management and Prompt Flows in Amazon Bedrock.\n",
    "\n",
    "[Amazon Bedrock Prompt Management](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management.html) streamlines the creation, evaluation, deployment, and sharing of prompts in the Amazon Bedrock console and via APIs in the SDK. This feature helps developers and business users obtain the best responses from foundation models for their specific use cases.\n",
    "\n",
    "[Amazon Bedrock Prompt Flows](https://docs.aws.amazon.com/bedrock/latest/userguide/flows.html) allows you to easily link multiple foundation models (FMs), prompts, and other AWS services, reducing development time and effort. It introduces a visual builder in the Amazon Bedrock console and a new set of APIs in the SDK, that simplifies the creation of complex generative AI workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making sure we have the lastest version of the Amazon Bedrock SDK, importing the libraries, and setting-up the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the Prompt Management and Flows features are part of the Bedrock Agent SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent = boto3.client(service_name = \"bedrock-agent\", region_name = \"us-west-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Management\n",
    "\n",
    "### Create and Manage Prompts\n",
    "\n",
    "Let's create a sample prompt, in this case for a simple translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_instruction = \"\"\"\n",
    "You are a friendly virtual assistant in a Primary school. \n",
    "Your job is to help students with their queries about school-related topics, such as History o Maths.\n",
    "\n",
    "Answer the following user query in a friendly and direct way: {{student_user_query}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model ID for the prompt\n",
    "model_id = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "model_variant = \"haiku-3-5-variant-001\"\n",
    "model_default_variant = \"haiku-3-5-variant-001\"\n",
    "\n",
    "# Create a prompt for the customer service use case\n",
    "# This prompt will be used to generate responses for customer service queries\n",
    "response = bedrock_agent.create_prompt(\n",
    "    name = \"ecomm-virtual-assistant-claude-3-5-haiku-v001\",\n",
    "    description = \"Prompt for out customer service virtual assistant\",\n",
    "    variants = [\n",
    "        {\n",
    "            \"inferenceConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"maxTokens\": 4096,\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"topP\": 0.5\n",
    "                }\n",
    "            },\n",
    "            \"modelId\": model_id,\n",
    "            \"name\": model_variant,\n",
    "            \"templateConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"inputVariables\": [\n",
    "                        {\n",
    "                            \"name\": \"student_user_query\"\n",
    "                        }\n",
    "\n",
    "                    ],\n",
    "                    \"text\": agent_instruction\n",
    "                }\n",
    "            },\n",
    "            \"templateType\": \"TEXT\"\n",
    "        }\n",
    "    ],\n",
    "    defaultVariant = model_default_variant\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "promptId = response[\"id\"]\n",
    "promptArn = response[\"arn\"]\n",
    "promptName = response[\"name\"]\n",
    "print(f\"Prompt ID: {promptId}\\nPrompt ARN: {promptArn}\\nPrompt Name: {promptName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a draft prompt, we can create versions from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt_version(\n",
    "    promptIdentifier = promptId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the list of prompts in our Prompt Library or catalog.\n",
    "\n",
    "Example: delete a prompt\n",
    "```\n",
    "response = bedrock_agent.delete_prompt(\n",
    "    promptIdentifier='4TX3FL6O17'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.list_prompts(\n",
    "    maxResults = 10\n",
    ")\n",
    "print(json.dumps(response[\"promptSummaries\"], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read the details of any of our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.get_prompt(\n",
    "    promptIdentifier = promptId,\n",
    "    promptVersion = \"1\"\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Todo en uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_name(response_data, name_to_find):\n",
    "    # Get the list of prompts from promptSummaries\n",
    "    prompts = response_data.get('promptSummaries', [])\n",
    "    \n",
    "    # Filter by name\n",
    "    return [item for item in prompts if item.get('name') == name_to_find]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDK initialization\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
    "\n",
    "def call_bedrock_with_prompt(user_prompt, prompt_name=\"ecomm-virtual-assistant-claude-3-5-haiku-v001\"):\n",
    "    # List available prompts\n",
    "    all_prompts = bedrock_agent.list_prompts()\n",
    "\n",
    "    # Get our prompt\n",
    "    filtered_prompt = filter_by_name(all_prompts, prompt_name)\n",
    "    filtered_prompt_arn = filtered_prompt[0][\"arn\"]\n",
    "\n",
    "    # Call the model using the prompt\n",
    "    response = bedrock_runtime.converse(\n",
    "      modelId=filtered_prompt_arn,\n",
    "      promptVariables={\n",
    "        'ecomm_user_query': {\n",
    "            'text': user_prompt\n",
    "        }\n",
    "      }   \n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User sample query\n",
    "user_prompt=\"Hi! Do you have video games?\"\n",
    "response = call_bedrock_with_prompt(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the response\n",
    "response_text = response['output']['message']['content'][0]['text']\n",
    "print(response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
